# The exponential rise in internet usage by people from all cultures and educational levels has made toxic online content become a significant problem in today's society. The automatic detection of toxic text content has significant challenges in differentiating between hate speech and offensive language. In this study, we suggest a method for classifying text into two categories: hate speech and non-hate speech. This project is focused on the detection of offensive data, and bully statements in shared data of social networks. In this project we will use the ‘Hate Speech and Offensive Language Dataset’ which has 31,935 tweets. The dataset was heavily skewed or imbalanced with 93% of tweets or 29,695 tweets containing non-hate labelled tweets and 7% or 2,240 tweets containing hate-labeled tweets data. Automated detection of these incidents requires the use of sophisticated and intelligent systems. Existing classical machine learning algorithms such as Random Forest, SVM, Naive Bayes with text cleaning for detecting cyberbullying have a lot of bottlenecks with less accuracy i.e., around 90%. We show how state-of-the-art deep learning NLP language models can overcome these bottlenecks with better accuracy and semantic understanding with hate and no-hate tweets.![image](https://user-images.githubusercontent.com/63062595/205798280-c4f2c54a-a1e1-498f-b02f-82f31e57706d.png)
